# voice-emotion-classification
**Voice Classification Project for Machine Learning Class 0451 with Catie Baxter and Otis Milliken**

## Abstract

## Motivation and Question
Our aim for this project is to convert the audio files in our data into vectorized information that we can use to classify the emotional tone used by the actor. We have a large data set which we will use to create predictive and exploratory models that will help us generate specific hypotheses regarding what elements of the audio indicate certain emotions. The target variable and other information about the file are included in the names of the audio files so we will also need to extract that information as we make our data usable. We then hope to use the tools and algorithms we have learned in class to build a model that will be able to classify emotion with a fair amount of accuracy. Speech emotion recognition falls under the category of affective computing which aims to investigate interactions between humans and computer and inform the optimization of these exchanges.([^1]) It is important to be able to classify emotion because ...

## Planned Deliverables
Our planned deliverables will include a Python package that contains all of the code we use to prepare our data, analyze the audio files, and build the model. We will also include a Jupyter notebook that illustrates the use of this package.

If everything works out, our full success would be if we are able to build a model that correctly classifies the emotion of an audio recording with a substancial amount of accuracy. We would also create clear and conscise functions that allow us to vectorize and visualize the audio files so that we fully understand why the model behaves as it does. 

Even if we are not able to accomplish everything we would like, our partial success would be if are able to build a model that correctly classifies the emotional tone of an audio file with an accuracy greater than randomly assigning categories

## Resources Required
We are using the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS). This dataset can be found on Zenodo at https://zenodo.org/records/1188976#.XrC7a5NKjOR. It contains 24 professional actors (12 male, 12 female) speaking identical phrases in calm, happy, sad, angry, fearful, surprise, and disgust tone in a neutral North American accent. The data files exist in audio format and have naming conventions that described the tone that they are. The tones were rated 10 times by 247 other individuals and put into emotional categories.  
While the data set is much larger and contains video and song files, we will only be examining the audio data files 

## Learning Goals
Otis:

Catie:

## Risk Statement

## Ethics Statement

## Timeline
Week 3:

Week 6:

## Works Cited
([^1]) Cambria, E., Das, D., Bandyopadhyay, S., Feraco, A. (2017). Affective Computing and Sentiment Analysis. In: Cambria, E., Das, D., Bandyopadhyay, S., Feraco, A. (eds) A Practical Guide to Sentiment Analysis. Socio-Affective Computing, vol 5. Springer, Cham. https://doi.org/10.1007/978-3-319-55394-8_1
